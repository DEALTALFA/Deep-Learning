{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "environmental-consolidation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'determine the client wil exit or not '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''determine the client wil exit or not '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "norman-investigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "partial-language",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv('Churn_Modelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "equivalent-seeker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9996</td>\n",
       "      <td>15606229</td>\n",
       "      <td>Obijiaku</td>\n",
       "      <td>771</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9997</td>\n",
       "      <td>15569892</td>\n",
       "      <td>Johnstone</td>\n",
       "      <td>516</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9998</td>\n",
       "      <td>15584532</td>\n",
       "      <td>Liu</td>\n",
       "      <td>709</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9999</td>\n",
       "      <td>15682355</td>\n",
       "      <td>Sabbatini</td>\n",
       "      <td>772</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>10000</td>\n",
       "      <td>15628319</td>\n",
       "      <td>Walker</td>\n",
       "      <td>792</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      RowNumber  CustomerId    Surname  CreditScore Geography  Gender  Age  \\\n",
       "0             1    15634602   Hargrave          619    France  Female   42   \n",
       "1             2    15647311       Hill          608     Spain  Female   41   \n",
       "2             3    15619304       Onio          502    France  Female   42   \n",
       "3             4    15701354       Boni          699    France  Female   39   \n",
       "4             5    15737888   Mitchell          850     Spain  Female   43   \n",
       "...         ...         ...        ...          ...       ...     ...  ...   \n",
       "9995       9996    15606229   Obijiaku          771    France    Male   39   \n",
       "9996       9997    15569892  Johnstone          516    France    Male   35   \n",
       "9997       9998    15584532        Liu          709    France  Female   36   \n",
       "9998       9999    15682355  Sabbatini          772   Germany    Male   42   \n",
       "9999      10000    15628319     Walker          792    France  Female   28   \n",
       "\n",
       "      Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0          2       0.00              1          1               1   \n",
       "1          1   83807.86              1          0               1   \n",
       "2          8  159660.80              3          1               0   \n",
       "3          1       0.00              2          0               0   \n",
       "4          2  125510.82              1          1               1   \n",
       "...      ...        ...            ...        ...             ...   \n",
       "9995       5       0.00              2          1               0   \n",
       "9996      10   57369.61              1          1               1   \n",
       "9997       7       0.00              1          0               1   \n",
       "9998       3   75075.31              2          1               0   \n",
       "9999       4  130142.79              1          1               0   \n",
       "\n",
       "      EstimatedSalary  Exited  \n",
       "0           101348.88       1  \n",
       "1           112542.58       0  \n",
       "2           113931.57       1  \n",
       "3            93826.63       0  \n",
       "4            79084.10       0  \n",
       "...               ...     ...  \n",
       "9995         96270.64       0  \n",
       "9996        101699.77       0  \n",
       "9997         42085.58       1  \n",
       "9998         92888.52       1  \n",
       "9999         38190.78       0  \n",
       "\n",
       "[10000 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#10000 x 14\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "handed-pencil",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed manually because RowNumber\tCustomerId\tSurname as they are not feature to effect leaving\n",
    "#'geography' and 'gender' we know its categorical value\n",
    "X=dataset[['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n",
    "       'IsActiveMember', 'EstimatedSalary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rental-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'exited' is our output \n",
    "y=dataset['Exited']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "exposed-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we converted string to integer internally called Labelencoder and to not get into dummyTrap we droped one column\n",
    "#by OneHotEncoding\n",
    "geo=pd.get_dummies(dataset['Geography'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "significant-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen=pd.get_dummies(dataset['Gender'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "impaired-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# axis=0 means adding row-wise\n",
    "X=pd.concat([X,geo,gen],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "permanent-medline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Germany</th>\n",
       "      <th>Spain</th>\n",
       "      <th>Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>771</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>516</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>709</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>772</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>792</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore  Age  Tenure    Balance  NumOfProducts  HasCrCard  \\\n",
       "0             619   42       2       0.00              1          1   \n",
       "1             608   41       1   83807.86              1          0   \n",
       "2             502   42       8  159660.80              3          1   \n",
       "3             699   39       1       0.00              2          0   \n",
       "4             850   43       2  125510.82              1          1   \n",
       "...           ...  ...     ...        ...            ...        ...   \n",
       "9995          771   39       5       0.00              2          1   \n",
       "9996          516   35      10   57369.61              1          1   \n",
       "9997          709   36       7       0.00              1          0   \n",
       "9998          772   42       3   75075.31              2          1   \n",
       "9999          792   28       4  130142.79              1          1   \n",
       "\n",
       "      IsActiveMember  EstimatedSalary  Germany  Spain  Male  \n",
       "0                  1        101348.88        0      0     0  \n",
       "1                  1        112542.58        0      1     0  \n",
       "2                  0        113931.57        0      0     0  \n",
       "3                  0         93826.63        0      0     0  \n",
       "4                  1         79084.10        0      1     0  \n",
       "...              ...              ...      ...    ...   ...  \n",
       "9995               0         96270.64        0      0     1  \n",
       "9996               1        101699.77        0      0     1  \n",
       "9997               1         42085.58        0      0     0  \n",
       "9998               0         92888.52        1      0     1  \n",
       "9999               0         38190.78        0      0     0  \n",
       "\n",
       "[10000 rows x 11 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we changed everything to integer\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "loose-apparel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "flush-guitar",
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide the dataset to check we have correct model train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "improving-supplier",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 11)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "consistent-production",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "provincial-destination",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()    #created brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "interested-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "offshore-merit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#added 1st layer with can also use input_shape=(11,) instead of input_dim(11) both are same\n",
    "#.we use 11 because we have 11 feature.\n",
    "model.add(Dense(units=8,activation='relu',input_dim=11))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "peripheral-gregory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 8)                 96        \n",
      "=================================================================\n",
      "Total params: 96\n",
      "Trainable params: 96\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()   #running the above again add more layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "viral-cancellation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'sequential_1',\n",
       " 'layers': [{'class_name': 'InputLayer',\n",
       "   'config': {'batch_input_shape': (None, 11),\n",
       "    'dtype': 'float32',\n",
       "    'sparse': False,\n",
       "    'ragged': False,\n",
       "    'name': 'dense_4_input'}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_4',\n",
       "    'trainable': True,\n",
       "    'batch_input_shape': (None, 11),\n",
       "    'dtype': 'float32',\n",
       "    'units': 8,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
       "     'config': {'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}}]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_config()  #more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "spiritual-worst",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=6,activation='relu'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "acting-smart",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'sequential_1',\n",
       " 'layers': [{'class_name': 'InputLayer',\n",
       "   'config': {'batch_input_shape': (None, 11),\n",
       "    'dtype': 'float32',\n",
       "    'sparse': False,\n",
       "    'ragged': False,\n",
       "    'name': 'dense_4_input'}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_4',\n",
       "    'trainable': True,\n",
       "    'batch_input_shape': (None, 11),\n",
       "    'dtype': 'float32',\n",
       "    'units': 8,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
       "     'config': {'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 6,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
       "     'config': {'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}}]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "copyrighted-marsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=6,activation='relu')) #3rd layer added where are 6 neuron will run 'relu' function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "advised-yukon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 8)                 96        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 54        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 42        \n",
      "=================================================================\n",
      "Total params: 192\n",
      "Trainable params: 192\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "parallel-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we know last output is binary classification 0 or 1 exited or not.So we use 1 netron so 1 o/p come out\n",
    "#binary classification: only 2 value:sigmoid function\n",
    "model.add(Dense(units=1,activation='sigmoid')) #4th layer   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "attached-spouse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 8)                 96        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 54        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 199\n",
      "Trainable params: 199\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "diagnostic-venue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'sequential_1',\n",
       " 'layers': [{'class_name': 'InputLayer',\n",
       "   'config': {'batch_input_shape': (None, 11),\n",
       "    'dtype': 'float32',\n",
       "    'sparse': False,\n",
       "    'ragged': False,\n",
       "    'name': 'dense_4_input'}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_4',\n",
       "    'trainable': True,\n",
       "    'batch_input_shape': (None, 11),\n",
       "    'dtype': 'float32',\n",
       "    'units': 8,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
       "     'config': {'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_5',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 6,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
       "     'config': {'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_6',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 6,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
       "     'config': {'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_7',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 1,\n",
       "    'activation': 'sigmoid',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
       "     'config': {'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}}]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_config() #we haven't give intializer but you can bydefault it has some function\n",
    "#kernel_initializer:GlorotUniform and bias_initializer:Zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "downtown-boutique",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "partial-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",optimizer=Adam(learning_rate=0.000001)) #training model with epochs without learning rate\n",
    "#will train wrong model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "anticipated-finding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.3895116 , -0.4226166 ,  0.45154423,  0.13835746,  0.50775546,\n",
       "          0.03955752,  0.31656986,  0.21646291],\n",
       "        [ 0.45586056,  0.5052889 ,  0.19410038,  0.04905248,  0.0448209 ,\n",
       "         -0.5548654 , -0.39831996, -0.13368827],\n",
       "        [ 0.33438337,  0.48901385,  0.03104377, -0.41511953, -0.4448275 ,\n",
       "         -0.4048879 ,  0.40241194, -0.02814686],\n",
       "        [-0.21696961,  0.09967029, -0.41249084, -0.33405685,  0.29379678,\n",
       "          0.40642154,  0.21470189, -0.33467275],\n",
       "        [-0.25089407,  0.02981061, -0.18019316, -0.4301606 ,  0.43367893,\n",
       "         -0.4709857 ,  0.47446102, -0.5226469 ],\n",
       "        [-0.04849499,  0.10234052, -0.46069592,  0.35228276, -0.3527363 ,\n",
       "         -0.1795724 , -0.12448856,  0.31948942],\n",
       "        [ 0.45016938,  0.43161446, -0.14705181, -0.03100556, -0.10048744,\n",
       "          0.5531655 , -0.2987949 , -0.48684433],\n",
       "        [ 0.01607728, -0.33071446,  0.18641114,  0.47795993, -0.4911353 ,\n",
       "         -0.45878673, -0.3301137 ,  0.2833907 ],\n",
       "        [ 0.24866414, -0.39216384,  0.3696463 ,  0.3914835 ,  0.10040557,\n",
       "          0.02253616,  0.05522645,  0.06795996],\n",
       "        [ 0.17209059,  0.22456402,  0.34546173, -0.49301824,  0.0949136 ,\n",
       "          0.5469671 ,  0.38972753, -0.3692903 ],\n",
       "        [ 0.30196273,  0.48913592,  0.29756176, -0.47293028,  0.533315  ,\n",
       "         -0.1383821 ,  0.193506  ,  0.4180277 ]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[-0.10312325, -0.15294844, -0.5832346 ,  0.30556405, -0.50130785,\n",
       "         -0.15590397],\n",
       "        [ 0.53215146, -0.2517557 , -0.48922184, -0.5373326 ,  0.28310674,\n",
       "          0.38765132],\n",
       "        [ 0.06318855,  0.12719762, -0.3685091 ,  0.15452701, -0.09691483,\n",
       "         -0.17141506],\n",
       "        [-0.27002412, -0.060682  , -0.08721244,  0.5551467 , -0.54672575,\n",
       "          0.10167247],\n",
       "        [-0.55216783,  0.01613855,  0.18174827, -0.64050925, -0.30471498,\n",
       "         -0.48050517],\n",
       "        [ 0.32347006, -0.19783607,  0.00267214, -0.03101391,  0.31969523,\n",
       "          0.2539997 ],\n",
       "        [-0.1075446 ,  0.3589331 ,  0.64976454,  0.14023525, -0.05482274,\n",
       "         -0.60372204],\n",
       "        [-0.20868921,  0.390983  ,  0.6214329 , -0.35930562, -0.39735785,\n",
       "          0.38478947]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[ 0.35693496, -0.50433594,  0.4881224 ,  0.14873606, -0.3502194 ,\n",
       "          0.27220643],\n",
       "        [ 0.34535176, -0.03229594,  0.6442283 , -0.19691443,  0.04352671,\n",
       "          0.21226668],\n",
       "        [-0.17651051, -0.68874454, -0.19584256, -0.6061027 , -0.4746428 ,\n",
       "          0.6306822 ],\n",
       "        [-0.43961623,  0.13229269,  0.11914325, -0.29115176, -0.36579975,\n",
       "          0.3567577 ],\n",
       "        [ 0.5477347 , -0.21860576, -0.48324126, -0.415342  , -0.03306216,\n",
       "          0.12320417],\n",
       "        [-0.5025582 , -0.01293302,  0.14475721,  0.5854904 , -0.19531894,\n",
       "          0.13867831]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[ 0.43902147],\n",
       "        [ 0.17085886],\n",
       "        [ 0.1984992 ],\n",
       "        [-0.26287127],\n",
       "        [-0.04583782],\n",
       "        [-0.0087496 ]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "following-mounting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 842.4478\n",
      "Epoch 2/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 833.5597\n",
      "Epoch 3/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 825.0045\n",
      "Epoch 4/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 816.7391\n",
      "Epoch 5/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 808.7092\n",
      "Epoch 6/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 800.9244\n",
      "Epoch 7/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 793.3318\n",
      "Epoch 8/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 785.8215\n",
      "Epoch 9/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 778.3710\n",
      "Epoch 10/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 770.9686\n",
      "Epoch 11/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 763.5992\n",
      "Epoch 12/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 756.2456\n",
      "Epoch 13/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 748.9100\n",
      "Epoch 14/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 741.5743\n",
      "Epoch 15/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 734.2563\n",
      "Epoch 16/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 726.9644\n",
      "Epoch 17/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 719.6844\n",
      "Epoch 18/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 712.4274\n",
      "Epoch 19/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 705.1831\n",
      "Epoch 20/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 697.9671\n",
      "Epoch 21/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 690.7554\n",
      "Epoch 22/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 683.5709\n",
      "Epoch 23/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 676.3978\n",
      "Epoch 24/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 669.2246\n",
      "Epoch 25/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 662.0577\n",
      "Epoch 26/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 654.8959\n",
      "Epoch 27/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 647.7440\n",
      "Epoch 28/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 640.6327\n",
      "Epoch 29/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 633.5520\n",
      "Epoch 30/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 626.5010\n",
      "Epoch 31/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 619.4736\n",
      "Epoch 32/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 612.4737\n",
      "Epoch 33/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 605.4973\n",
      "Epoch 34/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 598.5601\n",
      "Epoch 35/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 591.6689\n",
      "Epoch 36/200\n",
      "250/250 [==============================] - ETA: 0s - loss: 583.621 - 0s 1ms/step - loss: 584.7952\n",
      "Epoch 37/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 577.9521\n",
      "Epoch 38/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 571.1578\n",
      "Epoch 39/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 564.3918\n",
      "Epoch 40/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 557.6453\n",
      "Epoch 41/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 550.9330\n",
      "Epoch 42/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 544.2531\n",
      "Epoch 43/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 537.6068\n",
      "Epoch 44/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 531.0118\n",
      "Epoch 45/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 524.4324\n",
      "Epoch 46/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 517.8805\n",
      "Epoch 47/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 511.3627\n",
      "Epoch 48/200\n",
      "250/250 [==============================] - 0s 932us/step - loss: 504.8583\n",
      "Epoch 49/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 498.3794\n",
      "Epoch 50/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 491.9225\n",
      "Epoch 51/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 485.4839\n",
      "Epoch 52/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 479.0621\n",
      "Epoch 53/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 472.6664\n",
      "Epoch 54/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 466.3077\n",
      "Epoch 55/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 459.9770\n",
      "Epoch 56/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 453.6687\n",
      "Epoch 57/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 447.3927\n",
      "Epoch 58/200\n",
      "250/250 [==============================] - 0s 982us/step - loss: 441.1468\n",
      "Epoch 59/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 434.9352\n",
      "Epoch 60/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 428.7601\n",
      "Epoch 61/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 422.6078\n",
      "Epoch 62/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 416.4726\n",
      "Epoch 63/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 410.3686\n",
      "Epoch 64/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 404.2928\n",
      "Epoch 65/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 398.2355\n",
      "Epoch 66/200\n",
      "250/250 [==============================] - ETA: 0s - loss: 390.430 - 0s 1ms/step - loss: 392.1984\n",
      "Epoch 67/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 386.1884\n",
      "Epoch 68/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 380.2057\n",
      "Epoch 69/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 374.2476\n",
      "Epoch 70/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 368.3004\n",
      "Epoch 71/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 362.3681\n",
      "Epoch 72/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 356.4488\n",
      "Epoch 73/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 350.5453\n",
      "Epoch 74/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 344.6462\n",
      "Epoch 75/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 338.7623\n",
      "Epoch 76/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 332.9052\n",
      "Epoch 77/200\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 327.0854\n",
      "Epoch 78/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 321.2806\n",
      "Epoch 79/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 315.4943\n",
      "Epoch 80/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 309.7419\n",
      "Epoch 81/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 304.0142\n",
      "Epoch 82/200\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 298.3149\n",
      "Epoch 83/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 292.6295\n",
      "Epoch 84/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 286.9599\n",
      "Epoch 85/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 281.3043\n",
      "Epoch 86/200\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 275.6536\n",
      "Epoch 87/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 270.0113\n",
      "Epoch 88/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 264.3891\n",
      "Epoch 89/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 258.7841\n",
      "Epoch 90/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 253.1835\n",
      "Epoch 91/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 247.5983\n",
      "Epoch 92/200\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 242.0331\n",
      "Epoch 93/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 236.4988\n",
      "Epoch 94/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 230.9792\n",
      "Epoch 95/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 225.4841\n",
      "Epoch 96/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 1ms/step - loss: 220.1228\n",
      "Epoch 97/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 215.0358\n",
      "Epoch 98/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 210.3290\n",
      "Epoch 99/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 206.0563\n",
      "Epoch 100/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 202.1371\n",
      "Epoch 101/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 198.2576\n",
      "Epoch 102/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 194.3360\n",
      "Epoch 103/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 190.3896\n",
      "Epoch 104/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 186.4390\n",
      "Epoch 105/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 182.4769\n",
      "Epoch 106/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 178.5251\n",
      "Epoch 107/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 174.5830\n",
      "Epoch 108/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 170.6273\n",
      "Epoch 109/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 166.6811\n",
      "Epoch 110/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 162.7476\n",
      "Epoch 111/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 158.8260\n",
      "Epoch 112/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 154.9141\n",
      "Epoch 113/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 151.0374\n",
      "Epoch 114/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 147.1727\n",
      "Epoch 115/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 143.3151\n",
      "Epoch 116/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 139.4724\n",
      "Epoch 117/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 135.6572\n",
      "Epoch 118/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 131.8578\n",
      "Epoch 119/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 128.0646\n",
      "Epoch 120/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 124.2833\n",
      "Epoch 121/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 120.5143\n",
      "Epoch 122/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 116.7741\n",
      "Epoch 123/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 113.0592\n",
      "Epoch 124/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 109.3685\n",
      "Epoch 125/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 105.7018\n",
      "Epoch 126/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 102.0559\n",
      "Epoch 127/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 98.4284\n",
      "Epoch 128/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 94.8125\n",
      "Epoch 129/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 91.2111\n",
      "Epoch 130/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 87.6058\n",
      "Epoch 131/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 84.0071\n",
      "Epoch 132/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 80.4183\n",
      "Epoch 133/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 76.8418\n",
      "Epoch 134/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 73.2741\n",
      "Epoch 135/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 69.7125\n",
      "Epoch 136/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 66.1639\n",
      "Epoch 137/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 62.6182\n",
      "Epoch 138/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 59.0767\n",
      "Epoch 139/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 55.5369\n",
      "Epoch 140/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 52.0028\n",
      "Epoch 141/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 48.4705\n",
      "Epoch 142/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 44.9564\n",
      "Epoch 143/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 42.8483\n",
      "Epoch 144/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 42.5087\n",
      "Epoch 145/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 42.2380\n",
      "Epoch 146/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 41.9707\n",
      "Epoch 147/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 41.7027\n",
      "Epoch 148/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 41.4344\n",
      "Epoch 149/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 41.1614\n",
      "Epoch 150/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 40.8862\n",
      "Epoch 151/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 40.6072\n",
      "Epoch 152/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 40.3342\n",
      "Epoch 153/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 40.0638\n",
      "Epoch 154/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 39.7979\n",
      "Epoch 155/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 39.5339\n",
      "Epoch 156/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 39.2720\n",
      "Epoch 157/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 39.0139\n",
      "Epoch 158/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 38.7588\n",
      "Epoch 159/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 38.5063\n",
      "Epoch 160/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 38.2530\n",
      "Epoch 161/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 38.0007\n",
      "Epoch 162/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 37.7491\n",
      "Epoch 163/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 37.4989\n",
      "Epoch 164/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 37.2530\n",
      "Epoch 165/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 37.0122\n",
      "Epoch 166/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 36.7799\n",
      "Epoch 167/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 36.5556\n",
      "Epoch 168/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 36.3369\n",
      "Epoch 169/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 36.1211\n",
      "Epoch 170/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 35.9093\n",
      "Epoch 171/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 35.6989\n",
      "Epoch 172/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 35.4879\n",
      "Epoch 173/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 35.2785\n",
      "Epoch 174/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 35.0701\n",
      "Epoch 175/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 34.8626\n",
      "Epoch 176/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 34.6557\n",
      "Epoch 177/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 34.4498\n",
      "Epoch 178/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 34.2453\n",
      "Epoch 179/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 34.0422\n",
      "Epoch 180/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 33.8394\n",
      "Epoch 181/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 33.6372\n",
      "Epoch 182/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 33.4359\n",
      "Epoch 183/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 33.2368\n",
      "Epoch 184/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 33.0380: 0s - loss: 33.46\n",
      "Epoch 185/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 32.8406\n",
      "Epoch 186/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 32.6442\n",
      "Epoch 187/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 32.4467\n",
      "Epoch 188/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 32.2515\n",
      "Epoch 189/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 32.0566\n",
      "Epoch 190/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 31.8629\n",
      "Epoch 191/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 31.6695\n",
      "Epoch 192/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 1ms/step - loss: 31.4772\n",
      "Epoch 193/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 31.2851\n",
      "Epoch 194/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 31.0949\n",
      "Epoch 195/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 30.9067\n",
      "Epoch 196/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 30.7195\n",
      "Epoch 197/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 30.5342\n",
      "Epoch 198/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 30.3484\n",
      "Epoch 199/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 30.1641\n",
      "Epoch 200/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 29.9802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27706151880>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,epochs=200) #model trained. if using epochs more than cetrain no.\n",
    "#use adam learning rate else wrong model trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "beautiful-tennis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(pd.DataFrame(model.history.history)).plot() #shows history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-distributor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
